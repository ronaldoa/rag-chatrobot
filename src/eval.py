"""
Lightweight RAG evaluation runner - æ”¯æŒCSVæ ¼å¼

Usage:
    python -m src.eval --eval-path eval.csv --out results.csv --limit 100

Input file (CSV):
    Question,Answer,Label
    "What was...", "He noticed that...", "Timing"
"""
import argparse
import csv
import time
from pathlib import Path
from typing import List, Dict, Any

from .qa_service import qa_service


def load_eval_set_csv(path: Path) -> List[Dict[str, Any]]:
    """
    ä»CSVæ–‡ä»¶è¯»å–è¯„ä¼°æ•°æ®

    CSVæ ¼å¼:
        Question,Answer,Label
        "é—®é¢˜1", "ç­”æ¡ˆ1", "ç±»åˆ«1"
        "é—®é¢˜2", "ç­”æ¡ˆ2", "ç±»åˆ«2"
    """
    samples: List[Dict[str, Any]] = []

    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)

        for row_idx, row in enumerate(reader, start=1):
            # è·å–å­—æ®µï¼ˆå…¼å®¹ä¸åŒçš„åˆ—åï¼‰
            question = (
                row.get("Question") or
                row.get("question") or
                ""
            ).strip()

            answer = (
                row.get("Answer") or
                row.get("answer") or
                row.get("expected") or
                ""
            ).strip()

            label = (
                row.get("Label") or
                row.get("label") or
                row.get("category") or
                ""
            ).strip()

            if not question:
                print(f"âš ï¸  è·³è¿‡ç¬¬{row_idx}è¡Œï¼šé—®é¢˜ä¸ºç©º")
                continue

            samples.append({
                "question": question,
                "answer": answer,      # ä½œä¸ºæœŸæœ›ç­”æ¡ˆ
                "label": label,        # ä¿ç•™æ ‡ç­¾ä¿¡æ¯
                "row_idx": row_idx
            })

    return samples


def load_eval_set(path: Path) -> List[Dict[str, Any]]:
    """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼å¹¶åŠ è½½"""
    if path.suffix.lower() == '.csv':
        return load_eval_set_csv(path)
    elif path.suffix.lower() == '.jsonl':
        # åŸæœ‰çš„JSONLåŠ è½½é€»è¾‘
        import json
        samples: List[Dict[str, Any]] = []
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    samples.append(json.loads(line))
                except json.JSONDecodeError as e:
                    raise ValueError(f"Invalid JSON: {e}")
        return samples
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}ï¼Œè¯·ä½¿ç”¨.csvæˆ–.jsonl")


def evaluate(samples: List[Dict[str, Any]], limit: int | None = None) -> List[Dict[str, Any]]:
    """è¿è¡Œè¯„ä¼°"""
    qa_service.initialize()
    results: List[Dict[str, Any]] = []

    for idx, sample in enumerate(samples):
        if limit and idx >= limit:
            break

        question = sample.get("question", "").strip()
        expected = sample.get("answer", "").strip()
        label = sample.get("label", "")

        if not question:
            continue

        print(f"[{idx+1}/{len(samples) if not limit else min(limit, len(samples))}] {question[:60]}...")

        start = time.time()
        try:
            output = qa_service.ask(question)
            answer = output.get("answer", "")
            sources = output.get("sources", [])

            # è¯„ä¼°æŒ‡æ ‡
            # 1. ç®€å•å­ä¸²åŒ¹é…
            substring_match = bool(expected) and (expected.lower() in answer.lower())

            # 2. æ›´ç²¾ç¡®çš„åŒ¹é…ï¼šè®¡ç®—å…³é”®è¯è¦†ç›–ç‡
            keyword_coverage = calculate_keyword_coverage(expected, answer)

            latency = time.time() - start

            results.append({
                "idx": idx,
                "row_idx": sample.get("row_idx", idx),
                "label": label,
                "question": question,
                "expected": expected,
                "answer": answer,
                "substring_match": substring_match,
                "keyword_coverage": keyword_coverage,
                "latency_sec": round(latency, 3),
                "num_sources": len(sources),
                "sources": [s.get("source", "") for s in sources]
            })

            print(f"  âœ“ åŒ¹é…: {substring_match} | å…³é”®è¯: {keyword_coverage:.2f} | å»¶è¿Ÿ: {latency:.2f}s")

        except Exception as e:
            latency = time.time() - start
            results.append({
                "idx": idx,
                "row_idx": sample.get("row_idx", idx),
                "label": label,
                "question": question,
                "expected": expected,
                "answer": f"ERROR: {e}",
                "substring_match": False,
                "keyword_coverage": 0.0,
                "latency_sec": round(latency, 3),
                "num_sources": 0,
                "sources": []
            })
            print(f"  âŒ é”™è¯¯: {e}")

    return results


def calculate_keyword_coverage(expected: str, answer: str) -> float:
    """
    è®¡ç®—æœŸæœ›ç­”æ¡ˆçš„å…³é”®è¯åœ¨ç”Ÿæˆç­”æ¡ˆä¸­çš„è¦†ç›–ç‡

    Returns:
        0.0-1.0çš„åˆ†æ•°
    """
    if not expected or not answer:
        return 0.0

    # ç®€å•åˆ†è¯ï¼ˆå¯ä»¥ç”¨æ›´å¤æ‚çš„åˆ†è¯å™¨ï¼‰
    import re

    # æå–æœŸæœ›ç­”æ¡ˆçš„å…³é”®è¯ï¼ˆå»é™¤å¸¸è§åœç”¨è¯ï¼‰
    stopwords = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
        'he', 'she', 'it', 'they', 'his', 'her', 'their', 'this', 'that',
        'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†'
    }

    expected_words = set(re.findall(r'\w+', expected.lower()))
    expected_words = {w for w in expected_words if w not in stopwords and len(w) > 2}

    if not expected_words:
        return 0.0

    answer_lower = answer.lower()

    # è®¡ç®—è¦†ç›–çš„å…³é”®è¯æ•°
    covered = sum(1 for word in expected_words if word in answer_lower)

    return covered / len(expected_words)


def write_csv(rows: List[Dict[str, Any]], out_path: Path) -> None:
    """å†™å…¥CSVç»“æœ"""
    fieldnames = [
        "idx", "row_idx", "label", "question", "expected", "answer",
        "substring_match", "keyword_coverage", "latency_sec",
        "num_sources", "sources"
    ]

    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            # sourcesåˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
            if isinstance(row.get("sources"), list):
                row["sources"] = "; ".join(row["sources"])
            writer.writerow(row)


def summarize(rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ±‡æ€»ç»Ÿè®¡"""
    total = len(rows)
    if total == 0:
        return {}

    # åŸºç¡€ç»Ÿè®¡
    substring_matches = sum(1 for r in rows if r.get("substring_match"))
    avg_keyword_coverage = sum(r.get("keyword_coverage", 0.0) for r in rows) / total
    avg_latency = sum(r.get("latency_sec", 0.0) for r in rows) / total

    # æŒ‰Labelåˆ†ç»„ç»Ÿè®¡
    label_stats = {}
    for row in rows:
        label = row.get("label", "Unknown")
        if label not in label_stats:
            label_stats[label] = {
                "count": 0,
                "substring_matches": 0,
                "avg_coverage": 0.0,
                "avg_latency": 0.0
            }

        label_stats[label]["count"] += 1
        if row.get("substring_match"):
            label_stats[label]["substring_matches"] += 1
        label_stats[label]["avg_coverage"] += row.get("keyword_coverage", 0.0)
        label_stats[label]["avg_latency"] += row.get("latency_sec", 0.0)

    # è®¡ç®—å¹³å‡å€¼
    for label, stats in label_stats.items():
        count = stats["count"]
        stats["match_rate"] = round(stats["substring_matches"] / count, 3)
        stats["avg_coverage"] = round(stats["avg_coverage"] / count, 3)
        stats["avg_latency"] = round(stats["avg_latency"] / count, 3)

    return {
        "total": total,
        "substring_matches": substring_matches,
        "substring_match_rate": round(substring_matches / total, 3),
        "avg_keyword_coverage": round(avg_keyword_coverage, 3),
        "avg_latency_sec": round(avg_latency, 3),
        "by_label": label_stats
    }


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°RAGç³»ç»Ÿï¼ˆæ”¯æŒCSVå’ŒJSONLæ ¼å¼ï¼‰")
    parser.add_argument("--eval-path", type=Path, required=True,
                       help="è¯„ä¼°æ•°æ®è·¯å¾„ï¼ˆ.csvæˆ–.jsonlï¼‰")
    parser.add_argument("--out", type=Path, default=Path("eval_results.csv"),
                       help="ç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--limit", type=int, default=None,
                       help="é™åˆ¶è¯„ä¼°çš„æ ·æœ¬æ•°é‡")
    args = parser.parse_args()

    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {args.eval_path}")
    samples = load_eval_set(args.eval_path)
    print(f"âœ“ åŠ è½½äº† {len(samples)} ä¸ªé—®é¢˜\n")

    print("ğŸš€ å¼€å§‹è¯„ä¼°...\n")
    rows = evaluate(samples, limit=args.limit)

    print("\nğŸ“Š ç”Ÿæˆç»Ÿè®¡...")
    stats = summarize(rows)

    print("\n" + "="*60)
    print("ğŸ“ˆ è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"æ€»æ ·æœ¬æ•°:     {stats['total']}")
    print(f"å­ä¸²åŒ¹é…ç‡:   {stats['substring_match_rate']:.1%}")
    print(f"å…³é”®è¯è¦†ç›–:   {stats['avg_keyword_coverage']:.1%}")
    print(f"å¹³å‡å»¶è¿Ÿ:     {stats['avg_latency_sec']:.3f}s")

    if stats.get('by_label'):
        print("\næŒ‰æ ‡ç­¾åˆ†ç±»:")
        for label, label_stats in stats['by_label'].items():
            print(f"\n  ã€{label}ã€‘")
            print(f"    æ ·æœ¬æ•°:   {label_stats['count']}")
            print(f"    åŒ¹é…ç‡:   {label_stats['match_rate']:.1%}")
            print(f"    è¦†ç›–ç‡:   {label_stats['avg_coverage']:.1%}")
            print(f"    å¹³å‡å»¶è¿Ÿ: {label_stats['avg_latency']:.3f}s")

    print("="*60 + "\n")

    write_csv(rows, args.out)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.out}\n")


if __name__ == "__main__":
    main()