# ============ Model Settings ============
# GGUF model file
GGUF_MODEL_PATH=./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf

# Embedding model (HuggingFace model name or local path)
EMBEDDING_MODEL=./sentence-transformers/bge-base-en-v1.5

# Reranker model
RERANKER_MODEL=./BAAI/bge-reranker-base

# ============ Paths ============
DATA_PATH=./data
VECTOR_STORE_PATH=./vector_store
LOG_PATH=./logs
MODELS_PATH=./models

# ============ LLM Parameters ============
N_CTX=4096          # context window size
N_THREADS=8         # CPU threads (set to number of cores)
N_GPU_LAYERS=35     # GPU layers (0=CPU only, 35=all on GPU)
N_BATCH=512         # batch size
TEMPERATURE=0.3     # generation temperature (0.0-2.0; lower is more deterministic)
MAX_TOKENS=256      # max generated tokens
TOP_P=0.95          # Nucleus sampling
REPEAT_PENALTY=1.15 # repetition penalty (1.0=no penalty)

# ============ Retrieval Parameters ============
RETRIEVER_MODE=hybrid   # dense | bm25 | hybrid
HYBRID_WEIGHTS=0.5,0.5  # dense_weight,sparse_weight for hybrid mode
MULTI_QUERY=False       # Generate multiple query rewrites to improve recall
MULTI_QUERY_NUM=3       # Number of rewrites produced
INITIAL_K=40      # number of docs returned by FAISS coarse search
FINAL_K=5         # number of reranked docs returned
CHUNK_SIZE=1500   # document chunk size (characters)
CHUNK_OVERLAP=200 # overlap between chunks
SEMANTIC_CHUNKING=False              # enable semantic-based chunking
SEMANTIC_BREAKPOINT_PERCENTILE=95    # higher -> fewer, larger chunks
SEMANTIC_MIN_CHUNK_SIZE=200          # minimum chunk length when semantic splitting

# ============ Server Settings ============
SERVER_HOST=0.0.0.0
SERVER_PORT=7860
RELOAD=False   # auto reload in development
LOG_LEVEL=info # debug, info, warning, error
SHARE=False    # Gradio public share

# ============ Other Settings ============
# If using a HuggingFace mirror
# HF_ENDPOINT=https://hf-mirror.com
