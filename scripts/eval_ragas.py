#!/usr/bin/env python
"""
Step 2: 使用 RAGAS 对 predictions.jsonl 做离线评测

用法:
    python scripts/eval_ragas.py --pred-path eval/predictions.jsonl
"""

from __future__ import annotations

import argparse
import json
import sys
import os
from pathlib import Path
from typing import Any, Dict, List

REPO_ROOT = Path(__file__).resolve().parent.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from datasets import Dataset
from ragas import evaluate as ragas_evaluate
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline as hf_pipeline
from sentence_transformers import SentenceTransformer
from langchain.llms import HuggingFacePipeline

# 兼容不同版本 ragas 的 answer_relevance / answer_relevancy 命名
try:
    from ragas.metrics import (
        answer_relevance,
        context_precision,
        context_recall,
        faithfulness,
    )
except ImportError:  # 旧版本叫 answer_relevancy
    from ragas.metrics import (
        answer_relevancy as answer_relevance,  # type: ignore
        context_precision,
        context_recall,
        faithfulness,
    )


def load_predictions(path: Path) -> List[Dict[str, Any]]:
    data: List[Dict[str, Any]] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            data.append(obj)
    return data


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--pred-path", type=Path, required=True, help="predictions.jsonl generated by run_rag_predictions.py")
    parser.add_argument("--hf-judge-model", type=str, default="Qwen/Qwen2-1.5B-Instruct", help="HF model id/path for ragas judge.")
    parser.add_argument("--hf-device", type=str, default="auto", help="Device map for transformers: auto|cpu|cuda")
    parser.add_argument("--hf-max-new-tokens", type=int, default=128, help="Max new tokens for judge generation.")
    parser.add_argument("--embedding-model", type=str, default="BAAI/bge-base-en-v1.5", help="Embedding model for ragas.")
    args = parser.parse_args()

    records = load_predictions(args.pred_path)
    if not records:
        print(f"No records found in {args.pred_path}")
        return

    print(f"Loaded {len(records)} predictions from {args.pred_path}")

    questions: List[str] = []
    answers: List[str] = []
    contexts: List[List[str]] = []
    ground_truths: List[str] = []

    for r in records:
        questions.append(r.get("question", ""))
        answers.append(r.get("answer", ""))
        ctx_list = r.get("contexts", []) or [""]
        # Ensure contexts is a list of strings
        ctx_list = [c if isinstance(c, str) else str(c) for c in ctx_list]
        if not ctx_list:
            ctx_list = [""]
        contexts.append(ctx_list)
        ground_truths.append(r.get("ground_truth", ""))

    metrics = [answer_relevance, faithfulness, context_precision, context_recall]

    # Build HF judge LLM via transformers to avoid llama.cpp
    print("\nLoading HF judge model for ragas...")
    device_map = args.hf_device
    if device_map == "cpu":
        device_map = {"": "cpu"}
    judge_tok = AutoTokenizer.from_pretrained(args.hf_judge_model)
    judge_model = AutoModelForCausalLM.from_pretrained(
        args.hf_judge_model,
        torch_dtype="auto",
        device_map=device_map,
    )
    gen_pipe = hf_pipeline(
        "text-generation",
        model=judge_model,
        tokenizer=judge_tok,
        max_new_tokens=args.hf_max_new_tokens,
        temperature=0.1,
    )
    judge_llm = HuggingFacePipeline(pipeline=gen_pipe)

    # Embeddings for ragas
    if args.hf_device.startswith("cuda"):
        emb_device = args.hf_device
    elif args.hf_device == "cpu":
        emb_device = "cpu"
    else:
        emb_device = "cpu"
    embeddings_model = SentenceTransformer(args.embedding_model, device=emb_device)

    print("\nRunning RAGAS evaluation (sequential)...")
    per_sample: List[Dict[str, float]] = []
    rows_out: List[Dict[str, Any]] = []
    for i in range(len(questions)):
        sample_ds = Dataset.from_dict(
            {
                "question": [questions[i]],
                "answer": [answers[i]],
                "contexts": [contexts[i]],
                "ground_truth": [ground_truths[i]],
            }
        )
        res = ragas_evaluate(sample_ds, metrics=metrics, llm=judge_llm, embeddings=embeddings_model)
        scores_obj = getattr(res, "scores", res)
        if isinstance(scores_obj, dict):
            per_sample.append(scores_obj)
            rows_out.append(
                {
                    "question": questions[i],
                    "answer": answers[i],
                    "ground_truth": ground_truths[i],
                    "context_count": len(contexts[i]),
                    **scores_obj,
                }
            )

    # Aggregate averages
    agg: Dict[str, float] = {}
    for metric in ["answer_relevance", "faithfulness", "context_precision", "context_recall"]:
        vals = [s.get(metric) for s in per_sample if s.get(metric) is not None]
        if vals:
            agg[metric] = sum(vals) / len(vals)

    print("\nRAGAS METRICS (avg over samples)")
    print("=" * 40)
    for k, v in agg.items():
        print(f"{k:<20}: {v:.4f}")
    print("=" * 40)
    print(f"Samples evaluated: {len(per_sample)}")

    # Save per-sample scores
    out_dir = Path("eval_results")
    out_dir.mkdir(parents=True, exist_ok=True)
    if rows_out:
        try:
            import csv

            out_path = out_dir / "ragas_results.csv"
            fieldnames = list(rows_out[0].keys())
            with out_path.open("w", encoding="utf-8", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(rows_out)
            print(f"Per-sample ragas results saved to: {out_path}")
        except Exception as e:  # noqa: BLE001
            print(f"Could not save ragas_results.csv: {e}")


if __name__ == "__main__":
    main()
