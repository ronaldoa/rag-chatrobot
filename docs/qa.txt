Full Q&A flow (question → answer)
==================================

Example question: "How does Llama 3 perform on code generation tasks?"

Entry points
------------
- Gradio UI → `chat_function` → `qa_service.ask`
- REST API  → `POST /api/chat` → `qa_service.ask`

Stage 1: Retrieve context
-------------------------
1) Embed query  
   - `embeddings.embed_query(question)` → 384-dim vector
2) FAISS coarse search  
   - `vectorstore.similarity_search(..., k=20)` using cosine similarity
3) Rerank  
   - `reranker.predict([(question, doc_content), ...])`  
   - Sort by score, keep top-3 documents

Stage 2: Generate answer
------------------------
4) Build prompt  
   - Insert top documents into the prompt template with the user question
5) Tokenize and run LlamaCpp  
   - Streaming/loop: forward pass → sample next token (temperature=0.7, top_p=0.95) → append → update KV cache
6) Detokenize  
   - Convert token IDs back to text
7) Post-process  
   - Strip special tokens, trim whitespace, extract sources

Result
------
- Final answer text
- Sources (top-3 reranked documents with metadata)
- Served back to Gradio UI and API clients
